{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Page Rank with Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "Use page rank with map reduce to estimate strength of the florentine families. Find another graph dataset with more edges and nodes and test on that one as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\n",
      "Node ID: 0 - Node Name: ACCIAIUOL\n",
      "Node ID: 1 - Node Name: ALBIZZI\n",
      "Node ID: 2 - Node Name: BARBADORI\n",
      "Node ID: 3 - Node Name: BISCHERI\n",
      "Node ID: 4 - Node Name: CASTELLAN\n",
      "Node ID: 5 - Node Name: GINORI\n",
      "Node ID: 6 - Node Name: GUADAGNI\n",
      "Node ID: 7 - Node Name: LAMBERTES\n",
      "Node ID: 8 - Node Name: MEDICI\n",
      "Node ID: 9 - Node Name: PAZZI\n",
      "Node ID: 10 - Node Name: PERUZZI\n",
      "Node ID: 11 - Node Name: PUCCI\n",
      "Node ID: 12 - Node Name: RIDOLFI\n",
      "Node ID: 13 - Node Name: SALVIATI\n",
      "Node ID: 14 - Node Name: STROZZI\n",
      "Node ID: 15 - Node Name: TORNABUON\n"
     ]
    }
   ],
   "source": [
    "def parse_dynetml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    node_count = 0\n",
    "    nodes = {}\n",
    "    # Extract nodes and convert to ids so it is easier to process\n",
    "    # An inverse mapping would be done normally but since they \n",
    "    # are few nodes for testing I just printed out the mapping\n",
    "    # and compared the number to the name\n",
    "    for node_class in root.findall('.//nodeclass'):\n",
    "        for idx, node in enumerate(node_class.findall('.//node')):\n",
    "            nodes[node.attrib['id']] = idx    \n",
    "            node_count += 1\n",
    "\n",
    "    # Extract networks\n",
    "    networks = {}\n",
    "    for network in root.findall('.//network'):\n",
    "        links = [[] for _ in range(node_count)]\n",
    "        network_id = network.attrib['id']\n",
    "        linksXml = network.findall('.//link')\n",
    "        for link in linksXml :\n",
    "            # Check if the link exists\n",
    "            # If value is zero I assumed there is no relationship\n",
    "            # or we would have a fully connected graph\n",
    "            if float(link.attrib['value']) > 0:\n",
    "                source = nodes[link.attrib['source']]\n",
    "                target = nodes[link.attrib['target']]\n",
    "                links[source].append(target)\n",
    "        \n",
    "        node_count = len(links)\n",
    "        for i in range(len(links)):\n",
    "            links_temp = links[i]\n",
    "            links[i] = [1.0 / node_count, len(links[i]), links_temp]\n",
    "\n",
    "        networks[network_id] = links\n",
    "\n",
    "    return nodes, networks\n",
    "\n",
    "nodes, networks = parse_dynetml('padgett.xml')\n",
    "\n",
    "print(\"Nodes:\")\n",
    "for node_id, node_name in enumerate(nodes):\n",
    "    print(f\"Node ID: {node_id} - Node Name: {node_name}\")\n",
    "\n",
    "\n",
    "# In the end I use only the PADGB network which is for business relationships\n",
    "# between the florentine families. This is an arbitrary choice because I only\n",
    "# needed one of the two networks for testing\n",
    "with open('PADGB.txt', 'w') as f:\n",
    "    for network_id, network_links in networks.items():\n",
    "        if network_id == 'PADGM':\n",
    "            continue\n",
    "        for idx, el in enumerate(network_links):\n",
    "            f.write(f\"{idx}: {el}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below saves the code in it to a sepearate file. This is the actual map reduce job which we define using the mrjob library. The job can not be executed from the notebook and the runner I use later needs it to be in a separate file. Also the jobs read from files line by line. They can not receive input arguments. This is probably due to the fact them to be executed at separate cluster nodes and we expect big data divided between them that can not be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%file main.py\n",
    "from mrjob.job import MRJob\n",
    "import xml.etree.ElementTree as ET\n",
    "import ast\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "# We use global variables which is a bit hacky\n",
    "# But the goal of the exercise was to get used to map reduce\n",
    "# and get a feel for it. Because of this it was allowed\n",
    "dangling_sum = 0\n",
    "s = 0.85\n",
    "node_count = 0\n",
    "\n",
    "class MRPageRank(MRJob):\n",
    "\n",
    "    # Define two steps of the job\n",
    "    # First for each dangling node it sends its probability to a reducer\n",
    "    # Which we use to calculate the sum of probabilities for all dangling nodes\n",
    "    # In the second step we recalculate the page ranks\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.ip_mapper,\n",
    "                     reducer=self.ip_reducer),\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "    # If a node is dangling it sends its probability to node 1\n",
    "    # All nodes also send their data forward so it can be used \n",
    "    # in the second part of the job\n",
    "    def ip_mapper(self, _, line):\n",
    "        idx, line = line.split(':', 1)\n",
    "        key = int(idx)\n",
    "        global node_count, dangling_sum\n",
    "        dangling_sum = 0\n",
    "\n",
    "        # Here we caluclate the number of nodes\n",
    "        # and save it in the global variable node_count\n",
    "        # These values can be also passed through the job as\n",
    "        # well but this was simpler to implement\n",
    "        if key + 1 > node_count:\n",
    "            node_count = key + 1\n",
    "\n",
    "        parsed_line = ast.literal_eval(line)\n",
    "        p_self = parsed_line[0]\n",
    "        neighbor_count = parsed_line[1]\n",
    "        neighbors = parsed_line[2]\n",
    "\n",
    "        if neighbor_count == 0: yield 1, [p_self]\n",
    "\n",
    "        yield key, (p_self, neighbor_count, neighbors) \n",
    "    \n",
    "    # For each dangling node we add to the sum\n",
    "    # If a node is not dangling it would have sent\n",
    "    # all the data about itself so we just propagate it\n",
    "    # forward.\n",
    "    def ip_reducer(self, key, values):\n",
    "        global dangling_sum\n",
    "        for value in values:\n",
    "            if len(value) == 1:\n",
    "                dangling_sum += float(value[0])\n",
    "                continue\n",
    "            \n",
    "            yield key, value\n",
    "\n",
    "\n",
    "    # For each neighbor the method sends the conditional probability \n",
    "    # to go to it given we are for sure going to\n",
    "    # a neighboring node. We assume each neighbor has an equal chance\n",
    "    # to b chosen. The data is again propagated\n",
    "    # to ourselves so it can be used to caclulate the new page rank\n",
    "    def mapper(self, key, vals):\n",
    "        p_self, neighbor_count, neighbors = vals\n",
    "        for neigh in neighbors:\n",
    "            yield neigh, p_self/neighbor_count\n",
    "\n",
    "        yield key, (0.0, p_self, neighbor_count, neighbors)\n",
    "\n",
    "    # For each node we calculate the new page rank\n",
    "    # by first taking the sum of probabilities of neighbors\n",
    "    # coming to the node and then we use the page rank formula\n",
    "    # In the end we output the new page rank and the data about the node\n",
    "    # so it is easier to update the files with the data for the next iteartion\n",
    "    # and so I can calculate the change to see if we converged\n",
    "    def reducer(self, key, values):\n",
    "        global dangling_sum, s, node_count\n",
    "        probSum = 0\n",
    "        for val in values:\n",
    "            if not isinstance(val, float):\n",
    "                p_old = val[1]\n",
    "                neighbor_count = val[2]\n",
    "                neighbors = val[3]\n",
    "            else:\n",
    "                probSum += val\n",
    "        p_new = s * probSum + (s * dangling_sum  + 1.0 - s) / node_count\n",
    "        yield key, [p_new, neighbor_count, neighbors, p_old]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRPageRank.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below section I use mr_job runner to execute the python MRJob. Normally it is sent to a hadoop cluster so because of this the runner and the actual job script can not be in the same file. After I run the job in the same file as the input I update the values so that they reflect the new page ranks and then call the job again until we converge. To converge we want the sum of changes to the probabilites to become less than the tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, change: 5.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2, change: 0.725933\n",
      "iteration: 3, change: 0.284351\n",
      "iteration: 4, change: 0.213954\n",
      "iteration: 5, change: 0.151908\n",
      "iteration: 6, change: 0.120915\n",
      "iteration: 7, change: 0.086072\n",
      "iteration: 8, change: 0.069637\n",
      "iteration: 9, change: 0.050338\n",
      "iteration: 10, change: 0.041161\n",
      "iteration: 11, change: 0.030546\n",
      "iteration: 12, change: 0.025035\n",
      "iteration: 13, change: 0.019671\n",
      "iteration: 14, change: 0.015674\n",
      "iteration: 15, change: 0.012981\n",
      "iteration: 16, change: 0.010598\n",
      "iteration: 17, change: 0.009008\n",
      "iteration: 18, change: 0.007657\n",
      "iteration: 19, change: 0.006508\n",
      "iteration: 20, change: 0.005532\n",
      "iteration: 21, change: 0.004702\n",
      "iteration: 22, change: 0.003997\n",
      "iteration: 23, change: 0.003397\n",
      "iteration: 24, change: 0.002888\n",
      "iteration: 25, change: 0.002455\n",
      "iteration: 26, change: 0.002086\n",
      "iteration: 27, change: 0.001774\n",
      "iteration: 28, change: 0.001507\n",
      "iteration: 29, change: 0.001281\n",
      "iteration: 30, change: 0.001089\n",
      "iteration: 31, change: 0.000926\n",
      "iteration: 32, change: 0.000787\n",
      "iteration: 33, change: 0.000669\n",
      "iteration: 34, change: 0.000569\n",
      "iteration: 35, change: 0.000483\n",
      "iteration: 36, change: 0.000411\n",
      "iteration: 37, change: 0.000349\n",
      "iteration: 38, change: 0.000297\n",
      "iteration: 39, change: 0.000252\n",
      "iteration: 40, change: 0.000214\n",
      "iteration: 41, change: 0.000182\n",
      "iteration: 42, change: 0.000155\n",
      "iteration: 43, change: 0.000132\n",
      "iteration: 44, change: 0.000112\n",
      "iteration: 45, change: 0.000095\n",
      "iteration: 46, change: 0.000081\n",
      "iteration: 47, change: 0.000069\n",
      "iteration: 48, change: 0.000058\n",
      "iteration: 49, change: 0.000050\n",
      "iteration: 50, change: 0.000042\n",
      "iteration: 51, change: 0.000036\n",
      "iteration: 52, change: 0.000031\n",
      "iteration: 53, change: 0.000026\n",
      "iteration: 54, change: 0.000022\n",
      "iteration: 55, change: 0.000019\n",
      "iteration: 56, change: 0.000016\n",
      "iteration: 57, change: 0.000014\n",
      "iteration: 58, change: 0.000012\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from main import MRPageRank\n",
    "\n",
    "# Stop warnings for no configuration file\n",
    "logging.getLogger('mrjob').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def pagerank(fileName, tolerance = 0.00001):\n",
    "\n",
    "    mr_job = MRPageRank(args=[fileName])\n",
    "\n",
    "    change = 5\n",
    "    iteration = 0\n",
    "    while change > tolerance:\n",
    "        iteration += 1\n",
    "        check = 0\n",
    "        print(f'iteration: {iteration}, change: {format(change, \".6f\")}')\n",
    "        with mr_job.make_runner() as runner:\n",
    "            change = 0\n",
    "            runner.run()\n",
    "            # Update probabilities based on results\n",
    "            with open(fileName, 'w') as f:\n",
    "                for el in mr_job.parse_output(runner.cat_output()):\n",
    "                    old_rank = el[1][3]\n",
    "                    new_rank = el[1][0]\n",
    "                    check += new_rank\n",
    "                    change += abs(new_rank - old_rank)\n",
    "                    f.write(f'{el[0]}: {el[1][:3]}\\n')\n",
    "\n",
    "        if check != 1:\n",
    "            raise ValueError(f\"Error: {check}\")\n",
    "\n",
    "\n",
    "pagerank('PADGB.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the file with the results we can see the Medici family has the highest page rank which means they are the strongest which makes sense. On the other hand families with no bussiness relations/dangling nodes have the lowest ranks which is what would be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Large\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a web subgraph dataset for all the websites that point to epa.gov on this link: https://www.cs.cornell.edu/courses/cs685/2002fa/ in the Network datasets section. It has around 9000 edges and 5000 nodes. The dataset is self explanatory. Node rows start with n and the nodes have ids and a name. Edge rows start with e and are just source id and destination id. Below I parse it and run page rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def parse_graph(file_name):\n",
    "    graph = collections.defaultdict(list)\n",
    "    nodes = set()\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Take node lines and save all the node ids\n",
    "            if line.startswith('n'):\n",
    "                _, node_id, _ = line.split()\n",
    "                nodes.add(int(node_id))\n",
    "            # Take edge lines and add edges to graph\n",
    "            elif line.startswith('e'):\n",
    "                _, source, destination = line.split()\n",
    "                graph[int(source)].append(int(destination))\n",
    "    return graph, nodes\n",
    "\n",
    "def write_output(graph, nodes, output_file_name):\n",
    "    unique_nodes = len(nodes)\n",
    "    with open(output_file_name, 'w') as file:\n",
    "        for node in nodes:\n",
    "            neighbors = graph.get(node, [])\n",
    "            file.write(f'{node}: [{1.0/unique_nodes}, {len(neighbors)}, {neighbors}]\\n')\n",
    "\n",
    "graph, nodes = parse_graph('./web_subgraph.txt')\n",
    "write_output(graph, nodes, 'web-parsed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, change: 5.000000\n",
      "iteration: 2, change: 0.961941\n",
      "iteration: 3, change: 0.213348\n",
      "iteration: 4, change: 0.062813\n",
      "iteration: 5, change: 0.023719\n",
      "iteration: 6, change: 0.010736\n",
      "iteration: 7, change: 0.006698\n",
      "iteration: 8, change: 0.004646\n",
      "iteration: 9, change: 0.003602\n",
      "iteration: 10, change: 0.002838\n",
      "iteration: 11, change: 0.002317\n",
      "iteration: 12, change: 0.001903\n",
      "iteration: 13, change: 0.001586\n",
      "iteration: 14, change: 0.001326\n",
      "iteration: 15, change: 0.001116\n",
      "iteration: 16, change: 0.000941\n",
      "iteration: 17, change: 0.000796\n",
      "iteration: 18, change: 0.000673\n",
      "iteration: 19, change: 0.000571\n",
      "iteration: 20, change: 0.000484\n",
      "iteration: 21, change: 0.000411\n",
      "iteration: 22, change: 0.000349\n",
      "iteration: 23, change: 0.000296\n",
      "iteration: 24, change: 0.000251\n",
      "iteration: 25, change: 0.000213\n",
      "iteration: 26, change: 0.000181\n",
      "iteration: 27, change: 0.000154\n",
      "iteration: 28, change: 0.000131\n",
      "iteration: 29, change: 0.000111\n",
      "iteration: 30, change: 0.000094\n",
      "iteration: 31, change: 0.000080\n",
      "iteration: 32, change: 0.000068\n",
      "iteration: 33, change: 0.000058\n",
      "iteration: 34, change: 0.000049\n",
      "iteration: 35, change: 0.000042\n",
      "iteration: 36, change: 0.000036\n",
      "iteration: 37, change: 0.000030\n",
      "iteration: 38, change: 0.000026\n",
      "iteration: 39, change: 0.000022\n",
      "iteration: 40, change: 0.000019\n",
      "iteration: 41, change: 0.000016\n",
      "iteration: 42, change: 0.000013\n",
      "iteration: 43, change: 0.000011\n"
     ]
    }
   ],
   "source": [
    "pagerank('web-parsed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converge again in 43 iterations but it took around 1 minute and 20 seconds instead of 20 seconds. Since the dataset is bigger I just sorted seperately the results and got that https://www.usgs.gov/ is with the highest page rank. I do not know the site but this was my result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
